{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ea4f5b1",
   "metadata": {},
   "source": [
    "# Sentiment classification of Nepali Tweets using pre-trained models\n",
    "\n",
    "This notebook demonstrates how we can use state-of-the-art pre-trained models to classify the sentiments of English as well as Non-English (in this case, Nepali) Tweets from a specified user\n",
    "\n",
    "Here, I have fetched the last 200 tweets of [K.P. Sharma Oli](https://twitter.com/kpsharmaoli) and classified their sentiments by using 3 different models for a side-to-side comparison.\n",
    "\n",
    "**Sentiment classification models used in this notebook:**\n",
    "- [RoBERTa-base](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)\n",
    "- [Flair TextClassifier](https://github.com/flairNLP/flair)\n",
    "- [TextBlob Polarity](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787ce4e",
   "metadata": {},
   "source": [
    "## Specify the Twitter Username and Tweet Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4f0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'kpsharmaoli'   # Username of the target account\n",
    "limit=200   # No. of tweets to be fetched (starts from the latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f930c02",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c1518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Pre-processing\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import re\n",
    "\n",
    "# for Twitter API\n",
    "import tweepy, configparser\n",
    "\n",
    "# for Sentiment Analysis\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a1037d",
   "metadata": {},
   "source": [
    "## Fetch data from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc6f5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API stuff\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini') # config.ini stores the API Keys\n",
    "\n",
    "api_key = config['twitter']['api_key']\n",
    "api_key_secret = config['twitter']['api_key_secret']\n",
    "\n",
    "access_token = config['twitter']['access_token']\n",
    "access_token_secret = config['twitter']['access_token_secret']\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509ab1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching tweet data\n",
    "\n",
    "tweets = api.user_timeline(screen_name=user,\n",
    "    count = limit,\n",
    "    tweet_mode='extended')\n",
    "\n",
    "columns = ['Timestamp','Tweet']\n",
    "data = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    data.append([tweet.created_at, tweet.full_text])\n",
    "kpTweets = pd.DataFrame(data, columns=columns)\n",
    "print(f'Successfully fetched the last {limit} tweets of @{user}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd89fd6",
   "metadata": {},
   "source": [
    "## Preprocessing & Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b942227",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "def nptoen(text):\n",
    "    output = translator.translate(text, src='ne', dest='en').text  # change src to process texts of languages other than Nepali\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d01a0c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processed_tweets = []\n",
    "translated_tweets = []\n",
    "langs = []\n",
    "for tweet in kpTweets['Tweet']:\n",
    "    \n",
    "    tweet2 = re.sub(r'^RT[\\s]+', '', tweet)    # this will remove the old style retweet text \"RT\"\n",
    "    tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)    # this will remove hyperlinks\n",
    "    tweet2 = re.sub(r'#', '', tweet2)    # only removing the hash # sign from the word\n",
    "    \n",
    "    processed_tweets.append(tweet2)\n",
    "\n",
    "#     print(tweet2)\n",
    "    if len(tweet2)>5:\n",
    "        lang = detect(tweet2)\n",
    "    else:\n",
    "        lang = 'N/A'\n",
    "    \n",
    "    if lang == 'ne':\n",
    "        translated = nptoen(tweet2)\n",
    "    elif lang == 'en':\n",
    "        translated = tweet2\n",
    "    else:\n",
    "        translated = 'NaN'\n",
    "    translated_tweets.append(translated)\n",
    "    \n",
    "    \n",
    "    if len(tweet2)<2:\n",
    "        langs.append('NaN')\n",
    "        continue\n",
    "\n",
    "    langs.append(lang)\n",
    "\n",
    "kpTweets['Language'] = pd.Series(langs)\n",
    "kpTweets['Processed'] = pd.Series(processed_tweets)\n",
    "kpTweets['Translated'] = pd.Series(translated_tweets)\n",
    "kpTweets.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f55c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpTweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3785176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kpTweets.to_csv('kpTweetsPreprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdecac9",
   "metadata": {},
   "source": [
    "## RoBERTa\n",
    "\n",
    "This is [twitter-roBERTa-base-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment), a state-of-the-art NLP model trained on ~58M tweets and finetuned for sentiment analysis with the TweetEval benchmark.\n",
    "\n",
    "It uses Meta AI's [RoBERTa](https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/), a robustly optimized method for pretraining natural language processing (NLP) systems that improves on [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)), the self-supervised method released by Google in 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(roberta)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(roberta)\n",
    "\n",
    "labels = ['Negative', 'Neutral', 'Positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8bb7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(tweet_proc):\n",
    "    encoded_tweet = tokenizer(tweet_proc, return_tensors = 'pt')\n",
    "\n",
    "    # print(encoded_tweet)\n",
    "\n",
    "    # output = model(encoded_tweet['input_ids'], encoded_tweet['attention_mask'])\n",
    "    output = model(**encoded_tweet)\n",
    "\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    labelnscore = {}\n",
    "    for i in range(len(scores)):\n",
    "\n",
    "        l = labels[i]\n",
    "        s = scores[i]\n",
    "        labelnscore[l] = s\n",
    "\n",
    "    hah = pd.Series(labelnscore)\n",
    "\n",
    "    return [hah.idxmax(),hah.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998bfd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "confidence = []\n",
    "\n",
    "for tweet in kpTweets.Translated:\n",
    "    senti = sentiment(tweet)\n",
    "    sentiments.append(senti[0])\n",
    "    confidence.append(senti[1])\n",
    "\n",
    "kpTweets['RoBERTa-Sentiment'] = sentiments\n",
    "kpTweets['RoBERTa-Confidence'] = confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036e37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2471d25",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "[TextBlob](https://pypi.org/project/textblob/) is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\n",
    "\n",
    "In our case, we'll only be using its [sentiment analysis](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis) feature by calculating the polarity and subjectivity.\n",
    "\n",
    "The polarity score is a float within the range [-1.0, 1.0]. The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "\n",
    "for tweet in kpTweets.Translated:\n",
    "    polar = TextBlob(tweet).polarity\n",
    "    subjec = TextBlob(tweet).subjectivity\n",
    "\n",
    "    if polar > 0:\n",
    "        senti = 'Positive'\n",
    "    elif polar < 0:\n",
    "        senti = 'Negative'\n",
    "    else:\n",
    "        senti = 'Neutral'\n",
    "\n",
    "    sentiment.append(senti)\n",
    "    polarity.append(polar)\n",
    "    subjectivity.append(subjec)\n",
    "\n",
    "kpTweets['TB-Sentiment'] = sentiment\n",
    "kpTweets['TB-Polarity'] = polarity\n",
    "kpTweets['TB-Subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f51fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5d0f7",
   "metadata": {},
   "source": [
    "## Flair\n",
    "\n",
    "[Flair](https://github.com/flairNLP/flair) is a state-of-the-art NLP framework developed by Humboldt University of Berlin: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "# from segtok.segmenter import split_single\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(tweet):\n",
    "    text = Sentence(tweet)\n",
    "    # stacked_embeddings.embed(text)\n",
    "    classifier.predict(text)\n",
    "    return [text.labels[0].value, text.labels[0].score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f415d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = []\n",
    "confidence = []\n",
    "\n",
    "for tweet in kpTweets.Translated:\n",
    "    if type(tweet)==str:\n",
    "        sentiment = getSentiment(tweet)\n",
    "    else:\n",
    "        sentiment = ['NaN', 'NaN']\n",
    "    \n",
    "    sentiments.append(sentiment[0])\n",
    "    confidence.append(sentiment[1])\n",
    "\n",
    "kpTweets['Flair-Sentiment'] = pd.Series(sentiments)\n",
    "kpTweets['Flair-Confidence'] = pd.Series(confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67771c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpTweets.to_csv('kpTweets-output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "176.996px",
    "width": "325.994px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
